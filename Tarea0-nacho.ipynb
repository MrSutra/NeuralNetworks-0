{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-395/477 Redes Neuronales Artificiales I-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 0 - Introducción a Redes Neuronales </H3>\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "<H3 align='center'>Integrantes:</H3>\n",
    "<H3 align='center'>Fernanda Weiss Rol: 201373536-3</H3>\n",
    "<H3 align='center'>Ignacio Espinoza Rol: 201073527-3</H3>\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "**Temas**  \n",
    "* NNs por dentro: *back-propagation from scratch*.\n",
    "* Principales hiperparámetros de *back propagation*\n",
    "* Introducción a keras\n",
    "* Verificación numérica de las derivadas implementadas.\n",
    "\n",
    "** Formalidades **  \n",
    "* Equipos de trabajo de: 2 personas (*cada uno debe estar en condiciones de responder preguntas sobre cada punto del trabajo realizado*)\n",
    "* Se debe preparar un (breve) Jupyter/IPython notebook que explique la actividad realizada y las conclusiones del trabajo\n",
    "* Fecha de entrega: 30 de Marzo.\n",
    "* Formato de entrega: envı́o de link Github al correo electrónico del ayudante (*<francisco.mena.13@sansano.usm.cl>*) , incluyendo al profesor en copia (*<jnancu@inf.utfsm.cl>*). Por favor especificar el siguiente asunto: [Tarea0-INF395-I-2018]\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "\n",
    "La tarea se divide en cuatro secciones:\n",
    "\n",
    "[1.](#primero)   Back-propagation (BP) from *Scratch*   \n",
    "[2.](#segundo)   Comparar back-propagation (BP) de Keras  \n",
    "[3.](#tercero)   Verificación numérica del gradiente para una componente  \n",
    "[4.](#cuarto)   Implementar momentum como variante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "En el siguiente trabajo se implementará una red neuronal feed fordward con Back-propagation desde cero para resolver un problema de clasificación. Esta red se pondrá a prueba versus una implementación de una red neuronal proporcionada por [Keras](https://keras.io/), una API de alto nivel para redes neuronales escrita en Python.\n",
    "\n",
    "Para los entrenamientos de cada modelo se utilizará el dataset *[Iris](https://es.wikipedia.org/wiki/Iris_flor_conjunto_de_datos)*, el cual posee datos de 50 muestras de cada una de tres especies de la planta Iris: Iris setosa, Iris virginica e Iris versicolor. Cada entrada del dataset presenta mediciones del ancho y largo se sus sépalos y pétalos, además del tipo de planta en cuestión (clase o etiqueta).\n",
    "\n",
    "<img src=\"http://s5047.pcdn.co/wp-content/uploads/2015/04/iris_petal_sepal.png\" alt=\"Mediciones de una planta\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"primero\"></a>\n",
    "## 1. Back-propagation (BP) from *Scratch*\n",
    "\n",
    "BP (Back-propagation) es sin duda el paradigma dominante para entrenar redes neuronales *feed-forward*. En\n",
    "redes grandes, diseñadas para problemas reales, implementar BP eficientemente puede ser una tarea delicada\n",
    "que puede ser razonable delegar a una librerı́a especializada. Sin embargo, construir BP *from scratch* es muy\n",
    "útil con fines pedagógicos.\n",
    "\n",
    "$$ w^{(t+1)} \\leftarrow w^{(t)} - \\eta \\nabla_{w^{(t)}} Loss $$\n",
    " \n",
    "> a) Escriba un programa que permita entrenar una red FF con una arquitectura fija de 2 capa ocultas (con 32 neuronas en la primera capa y 16 en la segunda) y $K$ neuronas de salida, sin usar librerı́as, excepto eventualmente *numpy* para implementar operaciones básicas de algebra lineal. Por simplicidad, asuma funciones de activacion y error (*loss function*) diferenciables o subdiferenciables, además de tener la misma función de activación para las 2 capas ocultas. Adapte la arquitectura para un problema de clasificación de 3 clases, es decir la función de activación para la capa de salida debe ser **softmax** con número de neuronas $K$=3. Escriba funciones para:  \n",
    "* (i)  implementar el *forward pass*  \n",
    "* (ii) implementar el *backward pass*  \n",
    "* (iii) implementar la rutina principal de entrenamiento, adoptando, por simplicidad, la variante cíclica aleatorizada de SGD (un ejemplo a la vez, pero iterando cíclicamente sobre una configuración aleatoria del conjunto de entrenamiento) con una tasa de aprendizaje fija de 0.1 y número de ciclos fijos (*epochs*).\n",
    "\n",
    "> b) Escriba una función que permita hacer predicciones mediante la red FF definida anteriormente, sin usar librerı́as, excepto eventualmente *numpy*. Escriba una función vectorizada que implemente el forward pass sobre un conjunto de $n_{t}$ ejemplos, además de implementar la función de decisión, que a través de la salida de la red prediga el valor categórico de la clase (1, 2 o 3).\n",
    "\n",
    "> c) Demuestre que sus programas funcionan en un problema de clasificación. Para esto utilice el dataset **iris**, disponible a través de la librería __[*sklearn*](http://scikit-learn.org)__, el cual corresponde a la clasificación de distintos tipos de plantas de iris (3 clases) mediante 4 características reales continuas específicas de la planta, deberá entrenar (ajustar) los pesos de la red para realizar la tarea encomendada, variando las funciones de error (*loss*) entre *categorical cross entropy* y *mean squared error*, además de variar las funciones de activación para las 2 capas ocultas entre  ReLU (Rectifier Linear Unit) y la función logística (*sigmoid*). Especifique explícitamente las funciones anteriores, así como sus gradientes. Recuerde que debe transformar las etiquetas usando *one hot vectors*.\n",
    "<div class=\"alert alert-block alert-info\">Es una buena práctica el normalizar los datos antes de trabajar con el modelo</div>\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "X_train,y_train = load_iris(return_X_y=True)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "#transform target to one hot vector\n",
    "import keras\n",
    "y_onehot = keras.utils.to_categorical(y_train)\n",
    "```\n",
    "Para evaluar los resultados, construya un gráfico correspondiente al error de clasificación versus número\n",
    "de epochs, utilizando sólo el conjunto de entrenamiento (el objetivo de esta sección es familiarizarse\n",
    "con el algoritmo BP, no encontrar la mejor red). Grafique también la evolución de la función objetivo utilizada para el entrenamiento. Además de reportar el tiempo de entrenamiento mediante el algoritmo implementado.  \n",
    "Por último, para alguna configuración elegida, reporte la matriz de confusión mediante el uso de librerías como *sklearn* o *keras*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funciones de activación\n",
    "epsilon =  1e-15\n",
    "\n",
    "def sigmoid(x):\n",
    "    if x > 0:\n",
    "        x = np.maximum(epsilon, x) #si x es muy pequeño\n",
    "    if x < 0:\n",
    "        x = np.maximum(-600.,x)  #si x es muy negativo\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "    \n",
    "def gradient_sigmoid(x):\n",
    "    return sigmoid(x) * (1. - sigmoid(x))\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0) # only difference\n",
    "\n",
    "def gradient_softmax(x):\n",
    "    return softmax(x) * (1 - softmax(x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def gradient_relu(x):\n",
    "    return 1. * (x > 0)\n",
    "\n",
    "def loss_function():\n",
    "    pass\n",
    "\n",
    "def gradient_loss_function():\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def create_matrices(layers):\n",
    "    # matrices de salidas de neuronas\n",
    "    A = []\n",
    "    A_derivate = [] \n",
    "    # matrices de errores\n",
    "    error_outputs = []\n",
    "    error_weights = []\n",
    "    # pesos de la red\n",
    "    network_weights = []\n",
    "    \n",
    "    L = len(layers)\n",
    "    for l in range(L):\n",
    "        A.append(np.zeros(layers[l]))\n",
    "        A_derivate.append(np.zeros(layers[l]))\n",
    "        error_outputs.append(np.zeros(layers[l]))\n",
    "        \n",
    "    for l in range(L-1):\n",
    "        error_weights.append(np.zeros((layers[l],layers[l+1])))\n",
    "        network_weights.append(np.random.rand(layers[l],layers[l+1]))\n",
    "\n",
    "    return network_weights, error_weights, error_outputs, A, A_derivate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red neuronal creada\n",
      " - Cantidad de capas   : 3 [2, 3, 1]\n",
      " - Activación neuronas : sigmoid\n",
      " - Función de pérdida  : cross\n",
      "entrada [1, 2]\n",
      "capa 0\n",
      "-neurona 0\n",
      "soft 0.8589096182488509\n",
      "gradiente 0.12118388592846409\n",
      "-neurona 1\n",
      "soft 0.623058525074124\n",
      "gradiente 0.2348565994065812\n",
      "-neurona 2\n",
      "soft 0.8494122316737647\n",
      "gradiente 0.1279110923567594\n",
      "pesos [array([[0.5505776 , 0.25320507, 0.21197596],\n",
      "       [0.6278427 , 0.12467273, 0.75901128]]), array([[0.64863308],\n",
      "       [0.82776725],\n",
      "       [0.66889304]])]\n",
      "neuronas [[1, 2], array([0.85890962, 0.62305853, 0.84941223]), array([0.])]\n",
      "capa 2\n",
      "+capa 0\n",
      "peso [0.64863308 0.82776725 0.66889304]\n",
      "new [0.85890962 0.62305853 0.84941223]\n",
      "1.6410305709273159\n",
      "soft [1.]\n",
      "gradient soft [0.]\n",
      "neuronas [[1, 2], array([0.85890962, 0.62305853, 0.84941223]), array([1.])]\n",
      "gradiente [array([0., 0.]), array([0.12118389, 0.2348566 , 0.12791109]), array([0.])]\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork:\n",
    "    # layers: información de la cantidad de neuronas por capa\n",
    "    # activation: función de activación\n",
    "    # type_loss: función de pérdida\n",
    "    def __init__(self, layers, activation, type_loss):\n",
    "        self.L = len(layers) # cantidad capas\n",
    "        self.S = layers # cantidad de neuronas en cada capa\n",
    "        self.activation = activation \n",
    "        self.type_loss = type_loss\n",
    "        \n",
    "        # asignar función gradiente\n",
    "        if activation == sigmoid:\n",
    "            self.activation_gradient = gradient_sigmoid\n",
    "        if activation == relu:\n",
    "            self.activation_gradient = gradient_relu\n",
    "            \n",
    "\n",
    "        # crear arquitectura\n",
    "        self.weights, self. e_weights, self.e_output, self.neurons, self.n_gradient = create_matrices(layers)\n",
    "        print(\"Red neuronal creada\")\n",
    "        print(\" - Cantidad de capas   :\", len(layers), layers)\n",
    "        print(\" - Activación neuronas :\", activation.__name__)\n",
    "        print(\" - Función de pérdida  :\", type_loss)\n",
    "\n",
    "    \n",
    "    def forward_pass(self, example):\n",
    "        x = example[0].copy()\n",
    "        y = example[1].copy()\n",
    "        self.neurons[0] = x.copy()\n",
    "        print(\"entrada\", x)\n",
    "        for l in range(self.L - 2): # por cada capa\n",
    "            print(\"capa\", l)\n",
    "            for s in range(self.S[l+1]):\n",
    "                print(\"-neurona\",s)\n",
    "                aux = np.dot(self.weights[l][:,s], self.neurons[l])\n",
    "                self.neurons[l+1][s] = self.activation(aux).copy()\n",
    "                self.n_gradient[l+1][s] = self.activation_gradient(aux).copy()\n",
    "                print(\"soft\", self.neurons[l+1][s])\n",
    "                print(\"gradiente\", self.n_gradient[l+1][s])\n",
    "        print(\"pesos\",self.weights)\n",
    "        print(\"neuronas\",self.neurons)\n",
    "        print(\"capa\",self.L-1)\n",
    "        for s in range(self.S[self.L-1]): # capa final\n",
    "            print(\"+capa\", s)\n",
    "            print(\"peso\", self.weights[self.L-2][:,s])\n",
    "            print(\"new\",self.neurons[self.L-2])\n",
    "            aux = np.dot(self.weights[self.L-2][:,s], self.neurons[self.L-2])\n",
    "            self.neurons[self.L-1][s] = aux.copy()\n",
    "            print(self.neurons[self.L-1][s])\n",
    "        self.neurons[self.L-1] = softmax(self.neurons[self.L-1]).copy()    \n",
    "        self.n_gradient[self.L-1] = gradient_softmax(self.neurons[self.L-1]).copy()\n",
    "        print(\"soft\", self.neurons[self.L-1])\n",
    "        print(\"gradient soft\", self.n_gradient[self.L-1])\n",
    "        \n",
    "        \n",
    "    def backward_pass(self):\n",
    "        pass\n",
    "    \n",
    "    def train(X, Y, epoch, learning_rate):\n",
    "        pass\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "red = NeuralNetwork([2,3,1], activation=sigmoid, type_loss=\"cross\")\n",
    "\n",
    "red.forward_pass([[1,2],[3]])\n",
    "print(\"neuronas\", red.neurons)\n",
    "print(\"gradiente\", red.n_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
