{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-395/477 Redes Neuronales Artificiales I-2018 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 0 - Introducción a Redes Neuronales </H3>\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "**Temas**  \n",
    "* NNs por dentro: *back-propagation from scratch*.\n",
    "* Principales hiperparámetros de *back propagation*\n",
    "* Introducción a keras\n",
    "* Verificación numérica de las derivadas implementadas.\n",
    "\n",
    "** Formalidades **  \n",
    "* Equipos de trabajo de: 2 personas (*cada uno debe estar en condiciones de responder preguntas sobre cada punto del trabajo realizado*)\n",
    "* Se debe preparar un (breve) Jupyter/IPython notebook que explique la actividad realizada y las conclusiones del trabajo\n",
    "* Fecha de entrega: 30 de Marzo.\n",
    "* Formato de entrega: envı́o de link Github al correo electrónico del ayudante (*<francisco.mena.13@sansano.usm.cl>*) , incluyendo al profesor en copia (*<jnancu@inf.utfsm.cl>*). Por favor especificar el siguiente asunto: [Tarea0-INF395-I-2018]\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "\n",
    "#### Paquetes instalación\n",
    "Para poder trabajar en el curso se necesitará instalar librerías para Python, por lo que se recomienda instalarlas a través de *anaconda* (para Windows y sistemas Unix) en un entorno virtual, donde podrán elegir su versión de Python. Se instalarán librerías como __[*sklearn*](http://scikit-learn.org/stable/)__, una librería simple y de facil acceso para *data science*, __[*keras*](https://keras.io/)__ en su versión con GPU (para cálculo acelerado a través de la tarjeta gráfica), además de que ésta utiliza como *backend* *TensorFlow* o *Theano*, por lo que habrá que instalar alguno de éstos, además de las librerías básicas de *computer science* como *numpy*, *matplotlib*, *pandas*, además de claramente *jupyter*.\n",
    "\n",
    "* __[Descargar anacona](https://www.anaconda.com/download/#linux)__ \n",
    "\n",
    "* Luego de instalar Anaconda y tenerla en el *path* de su computador crear un entorno virtual: \n",
    "```\n",
    "conda create -n redesneuronales python=version\n",
    "```\n",
    "\n",
    "con *version*, la version de Python que desea utilizar. Si está en Windows, se recomienda Python 3 debido a dependencias con una de las librerías a utilziar.\n",
    " \n",
    "* Acceder al ambiente creado \n",
    "```\n",
    "source activate redesneuronales\n",
    "```\n",
    "\n",
    "* Instalar los paquetes a utilizar\n",
    "```\n",
    "conda install jupyter sklearn numpy pandas matplotlib keras-gpu tensorflow-gpu \n",
    "```\n",
    "\n",
    "* Para salir del entorno\n",
    "```\n",
    "source deactivate redesneuronales\n",
    "```\n",
    "<hr style=\"height:1px;border:none\"/>\n",
    "\n",
    "\n",
    "La tarea se divide en cuatro secciones:\n",
    "\n",
    "[1.](#primero)   Back-propagation (BP) from *Scratch*   \n",
    "[2.](#segundo)   Comparar back-propagation (BP) de Keras  \n",
    "[3.](#tercero)   Verificación numérica del gradiente para una componente  \n",
    "[4.](#cuarto)   Implementar momentum como variante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"primero\"></a>\n",
    "## 1. Back-propagation (BP) from *Scratch*\n",
    "\n",
    "BP (Back-propagation) es sin duda el paradigma dominante para entrenar redes neuronales *feed-forward*. En\n",
    "redes grandes, diseñadas para problemas reales, implementar BP eficientemente puede ser una tarea delicada\n",
    "que puede ser razonable delegar a una librerı́a especializada. Sin embargo, construir BP *from scratch* es muy\n",
    "útil con fines pedagógicos.\n",
    "\n",
    "$$ w^{(t+1)} \\leftarrow w^{(t)} - \\eta \\nabla_{w^{(t)}} Loss $$\n",
    " \n",
    "> a) Escriba un programa que permita entrenar una red FF con una arquitectura fija de 2 capa ocultas (con 32 neuronas en la primera capa y 16 en la segunda) y $K$ neuronas de salida, sin usar librerı́as, excepto eventualmente *numpy* para implementar operaciones básicas de algebra lineal. Por simplicidad, asuma funciones de activacion y error (*loss function*) diferenciables o subdiferenciables, además de tener la misma función de activación para las 2 capas ocultas. Adapte la arquitectura para un problema de clasificación de 3 clases, es decir la función de activación para la capa de salida debe ser **softmax** con número de neuronas $K$=3. Escriba funciones para:  \n",
    "* (i)  implementar el *forward pass*  \n",
    "* (ii) implementar el *backward pass*  \n",
    "* (iii) implementar la rutina principal de entrenamiento, adoptando, por simplicidad, la variante cíclica aleatorizada de SGD (un ejemplo a la vez, pero iterando cíclicamente sobre una configuración aleatoria del conjunto de entrenamiento) con una tasa de aprendizaje fija de 0.1 y número de ciclos fijos (*epochs*).\n",
    "\n",
    "> b) Escriba una función que permita hacer predicciones mediante la red FF definida anteriormente, sin usar librerı́as, excepto eventualmente *numpy*. Escriba una función vectorizada que implemente el forward pass sobre un conjunto de $n_{t}$ ejemplos, además de implementar la función de decisión, que a través de la salida de la red prediga el valor categórico de la clase (1, 2 o 3).\n",
    "\n",
    "> c) Demuestre que sus programas funcionan en un problema de clasificación. Para esto utilice el dataset **iris**, disponible a través de la librería __[*sklearn*](http://scikit-learn.org)__, el cual corresponde a la clasificación de distintos tipos de plantas de iris (3 clases) mediante 4 características reales continuas específicas de la planta, deberá entrenar (ajustar) los pesos de la red para realizar la tarea encomendada, variando las funciones de error (*loss*) entre *categorical cross entropy* y *mean squared error*, además de variar las funciones de activación para las 2 capas ocultas entre  ReLU (Rectifier Linear Unit) y la función logística (*sigmoid*). Especifique explícitamente las funciones anteriores, así como sus gradientes. Recuerde que debe transformar las etiquetas usando *one hot vectors*.\n",
    "<div class=\"alert alert-block alert-info\">Es una buena práctica el normalizar los datos antes de trabajar con el modelo</div>\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "X_train,y_train = load_iris(return_X_y=True)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "#transform target to one hot vector\n",
    "import keras\n",
    "y_onehot = keras.utils.to_categorical(y_train)\n",
    "```\n",
    "Para evaluar los resultados, construya un gráfico correspondiente al error de clasificación versus número\n",
    "de epochs, utilizando sólo el conjunto de entrenamiento (el objetivo de esta sección es familiarizarse\n",
    "con el algoritmo BP, no encontrar la mejor red). Grafique también la evolución de la función objetivo utilizada para el entrenamiento. Además de reportar el tiempo de entrenamiento mediante el algoritmo implementado.  \n",
    "Por último, para alguna configuración elegida, reporte la matriz de confusión mediante el uso de librerías como *sklearn* o *keras*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nacho/anaconda3/envs/redesneuronales/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "X_train,y_train = load_iris(return_X_y=True)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "#transform target to one hot vector\n",
    "import keras\n",
    "y_onehot = keras.utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En una red neuronal Feed Forward (FF) se cuenta con la capa de entrada, capas ocultas y la capa de salida, dentro de cada capa existe una cantidad variable de neuronas. Toda la información se mueve solo hacia adelante, es decir, desde la capa de entrada, luego hacia las capas ocultas en orden y finalmente la capa de salida. Cada capa posee una función de activación, que generalmente, en las capas ocultas que es una transformación aprendida por cada neurona de la combinación lineal y no linealidad. \n",
    "\n",
    "Para este caso, se trabajara una red Feed Forward con:\n",
    "* Dos capas ocultas: 32 neuronas la primera y 16 neuronas la segunda.\n",
    "* La capa de salida posee 3 neuronas ,debido a que se desea clasificar entre tres clases.\n",
    "* La función de activación de las capas ocultas será la función sigmoidal y para la capa de salida una función sofmax.\n",
    "* La función de pérdida será inicialmente con una función cross entropy.\n",
    "\n",
    "Para entrenar una red FF se deben realizar principalmente tres pasos por cada ejemplo del entrenamiento, y estos a su vez se van iterando:\n",
    "\n",
    "1) El primer paso es implementar el *forward pass*, el cual realiza la predicción de la red. Este paso se le entrega un input a la red, el cual se va transformando a medida que pasan por las capas, hasta llegar a la última capa y entregar el output correspondiente. Desde la capa de entrada el input pasa a la primera capa oculta, en donde se realiza una combinación lineal de las neuronas con sus respectivos pesos de la capa de entrada en cada una de las 32 neuronas, y luego se le aplica la función de activación sigmoidal. Lo mismo ocurre con la segunda capa oculta, en que se realiza una combinación lineal de la capa anterior por cada neurona y luego se aplica la función de activación sigmoidal. Finalmente, para la ultima capa se realiza el mismo procedimiento, solo cambia que la función de activación es la sofmax.\n",
    "\n",
    "2) A continuación se mide el error y la función de pérdida, obtenida del resultado anterior del forward pass\n",
    "\n",
    "3) Finalmente se debe implementar el *backward pass*, el cual corrige los pesos obtenidos en cada capa según la pérdida observada en el paso anterior. Se parte de la última capa hasta llegar a la primera. Aquí se modifican los pesos disminuyendo la tasa de aprendizaje entregada como parámetro por la derivada parcial del error respecto al peso correspondiente.\n",
    "\n",
    "Como se mencionó anteriormente, los pasos se deben repetir la cantidad de veces como ejemplos de entrenamiento existan, es decir aplicar el método de SGD, donde se evalúa ejemplo por ejemplo.\n",
    "\n",
    "A continuación se encuentra la implementación que esta comentada con lo realizado en cada paso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "epsilon =  1e-15\n",
    "#----funcion de activacion y gradiente\n",
    "def sigmoid(x):\n",
    "    if x > 0:\n",
    "        x = np.maximum(epsilon, x) #si x es muy pequeño\n",
    "    #if x < 0:\n",
    "    #    x = np.maximum(-600,x)  #si x es muy negativo\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "def gradient_sigmoid(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def softmax(x):\n",
    "    retorno = np.zeros(len(x))\n",
    "    suma =0.0\n",
    "    for i in x:\n",
    "        valor = np.maximum(epsilon,i)\n",
    "        suma+=np.exp(valor)\n",
    "    for i in range(len(x)):\n",
    "        valor = np.maximum(epsilon,x[i])\n",
    "        retorno[i] = np.exp(valor)/suma #salida normalizada\n",
    "    return retorno\n",
    "\n",
    "def gradient_softmax(x):\n",
    "    return softmax(x) * (1 - softmax(x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def gradient_relu(x):\n",
    "    return 1. * (x > 0)\n",
    "\n",
    "#---funcion de perdida y gradiente\n",
    "def loss_function(y, ypred, type_error=\"cross_entropy\"):\n",
    "    ypred = np.maximum(epsilon, ypred)\n",
    "    if type_error == \"cross_entropy\":\n",
    "        return -1.0 * (y * np.log(ypred) + np.subtract(1, y) * np.log(np.subtract(1, ypred)))\n",
    "    else:\n",
    "        # quadratic error\n",
    "        return np.pow(y - ypred, 2) / 2\n",
    "\n",
    "def gradient_loss_function(y, ypred, type_error=\"cross_entropy\"):\n",
    "    ypred = np.maximum(epsilon, ypred)\n",
    "    if type_error == \"cross_entropy\":\n",
    "        return -1.0* (y - ypred) / (ypred * np.subtract(1, ypred))\n",
    "    else:\n",
    "        #error cuadratico\n",
    "        return y-ypred\n",
    "    \n",
    "def init_network(n_input, hidden_neurons, n_output, L):\n",
    "    S = [n_input] + hidden_neurons + [n_output]\n",
    "    \n",
    "    #inicializan matrices de salidas de neuronas\n",
    "    A = []\n",
    "    A_derivate = [] \n",
    "    #inicializa matrices que guardan los errores\n",
    "    error_outputs = []\n",
    "    error_weights = []\n",
    "    #inicializa pesos de la red\n",
    "    network_weights = []\n",
    "    \n",
    "    for l in range(L):\n",
    "        A.append(np.zeros(S[l])) #salida de capa l\n",
    "        A_derivate.append(np.zeros(S[l]))       #salida derivada de capa l\n",
    "        error_outputs.append(np.zeros(S[l]))\n",
    "        \n",
    "    for l in range(L-1):\n",
    "        error_weights.append(np.zeros((S[l],S[l+1])))\n",
    "        network_weights.append(np.random.rand(S[l],S[l+1]))\n",
    "        \n",
    "    return S, network_weights, error_weights, error_outputs, A, A_derivate\n",
    "\n",
    "  \n",
    "class neural_network:\n",
    "    \n",
    "    def __init__(self, layers, type_loss):\n",
    "        self.n_layers = len(layers)\n",
    "        self.n_input = layers[0]\n",
    "        self.n_output = layers[-1]\n",
    "        self.n_hidden = layers[1:-1]\n",
    "        self.L = 2 + len(self.n_hidden)  #numero de capas\n",
    "        self.type_loss = type_loss\n",
    "        \n",
    "        # create architecture\n",
    "        self.S, self.weights, self.e_weights, self.e_output, self.neuron, self.neuron_gradient = init_network(self.n_input,\n",
    "                                                                                self.n_hidden, self.n_output, self.L)\n",
    "        \n",
    "        \n",
    "    #i) calcular forward pass, predicción de la red, actualizan las salidas\n",
    "    def forward_pass(self, example):\n",
    "        x = example[0]\n",
    "        y = example[1]\n",
    "        self.neuron[0] = x.copy()\n",
    "        for l in range(self.L-1): #por cada transicion entre capas {0,L-2}          \n",
    "            for s in range(self.S[l+1]): #por cada neurona de la capa siguiente\n",
    "                aux = np.dot(self.weights[l][:,s], self.neuron[l])\n",
    "                self.neuron[l+1][s] = sigmoid(aux)\n",
    "                self.neuron_gradient[l+1][s] = gradient_sigmoid(aux)\n",
    "        #ultima capa softmax\n",
    "        self.neuron[self.L-1] = softmax(self.neuron[self.L-1])  #prediccion\n",
    "        self.neuron_gradient[self.L-1] = gradient_softmax(self.neuron[self.L-1])\n",
    "    \n",
    "    \n",
    "    #ii) calcular backward pass, modifica la red considerando el error observado\n",
    "    def backward_pass(self, example, learning_rate):\n",
    "        x = example[0]\n",
    "        y = example[1]\n",
    "        \n",
    "        #calcula el error en la ultima capa\n",
    "        for s in range(self.S[self.L-1]): \n",
    "            #arquitectura con multiples salidas (y es un vector)\n",
    "            ypred = self.neuron[self.L-1]  #.copy()\n",
    "            self.e_output[self.L-1][s] = gradient_loss_function(y[s], ypred[s])\n",
    "            self.e_weights[self.L-2][:,s] = np.dot(self.e_output[self.L-1][s],\n",
    "                                                   np.dot(self.neuron_gradient[self.L-1][s],\n",
    "                                                   self.neuron[self.L-2]))\n",
    "\n",
    "            #actualizar pesos en vector\n",
    "            self.weights[self.L-2][:,s] = self.weights[self.L-2][:,s] - learning_rate*self.e_weights[self.L-2][:,s]\n",
    "        #calcula el error recursivamente\n",
    "        for l in np.arange(self.L-2,0,-1): #desde la penultima capa hasta la segunda {L-2,1}\n",
    "            for s in range(self.S[l]): #para cada neurona en la capa l\n",
    "                self.e_output[l][s] = np.sum(self.e_output[l+1])\n",
    "                self.e_weights[l-1][:,s] =  np.dot( self.e_output[l][s], np.dot(self.neuron_gradient[l][s], \n",
    "                                                                                self.neuron[l-1]) )\n",
    "\n",
    "                #actualizar pesos en vector\n",
    "                self.weights[l-1][:,s] = self.weights[l-1][:,s] - learning_rate*self.e_weights[l-1][:,s]\n",
    "            \n",
    "\n",
    "\n",
    "    #iii) estrategia principal de entrenamiento\n",
    "    def train(self, X, Y, epoch, learning_rate):\n",
    "        data = list(zip(X, Y)) \n",
    "        np.random.shuffle(data)  #desordenar los ejemplos\n",
    "        \n",
    "        loss_epochs = []\n",
    "        error_epochs = []\n",
    "        for i in range(epoch):\n",
    "            loss = []\n",
    "            error = []\n",
    "            #print(i)\n",
    "            #print(len(list(data)))\n",
    "            for example in data: #con sgd un ejemplo a la vez\n",
    "                self.forward_pass(example)\n",
    "                #measure_error(network, example)\n",
    "                \n",
    "                #medir el error y loss\n",
    "                ypred = self.neuron[self.L-1].copy()\n",
    "                #print(\"example\")\n",
    "                #print(example[1])\n",
    "                #print(ypred)\n",
    "                loss.append(loss_function(example[1], ypred))\n",
    "                error.append( 1 - example[1][np.argmax(ypred)] )\n",
    "                \n",
    "                self.backward_pass(example, learning_rate)\n",
    "            \n",
    "            #print(loss)\n",
    "            loss_epochs.append(np.mean(loss))\n",
    "            error_epochs.append(np.mean(error))\n",
    "            #print(\"loss \" + str(loss_epochs[-1]))\n",
    "            \n",
    "        return loss_epochs, error_epochs\n",
    "        \n",
    "\n",
    "n_inputs = 10\n",
    "info = [n_inputs, 32, 16, 3]\n",
    "nn = neural_network(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Escriba una función que permita hacer predicciones mediante la red FF definida anteriormente, sin usar librerı́as, excepto eventualmente numpy. Escriba una función vectorizada que implemente el forward pass sobre un conjunto de  ntnt  ejemplos, además de implementar la función de decisión, que a través de la salida de la red prediga el valor categórico de la clase (1, 2 o 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FF  = red ya entrenada\n",
    "def predecir(X_test,y_test,FF):\n",
    "    n_test = len(y_test)\n",
    "    print(\"numero de ejemplos: \" + n_test)\n",
    "    \n",
    "    predicciones = []\n",
    "    for xi,yi in zip(X_test,y_test): #para n_test ejemplos\n",
    "        y_hat = forward_pass(xi,yi,FF.L,FF.WM, FF.A,FF.A_der,FF.S)\n",
    "        predicciones.append(y_hat)\n",
    "    return predicciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Demuestre que sus programas funcionan en un problema de clasificación. Para esto utilice el dataset iris, disponible a través de la librería sklearn, el cual corresponde a la clasificación de distintos tipos de plantas de iris (3 clases) mediante 4 características reales continuas específicas de la planta, deberá entrenar (ajustar) los pesos de la red para realizar la tarea encomendada, variando las funciones de error (loss) entre categorical cross entropy y mean squared error, además de variar las funciones de activación para las 2 capas ocultas entre ReLU (Rectifier Linear Unit) y la función logística (sigmoid). Especifique explícitamente las funciones anteriores, así como sus gradientes. Recuerde que debe transformar las etiquetas usando one hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "import keras\n",
    "\n",
    "X_train,y_train = load_iris(return_X_y=True)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "#transform target to one hot vector\n",
    "y_onehot = keras.utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.00681170e-01  1.03205722e+00 -1.34127240e+00 -1.31297673e+00]\n",
      " [-1.14301691e+00 -1.24957601e-01 -1.34127240e+00 -1.31297673e+00]\n",
      " [-1.38535265e+00  3.37848329e-01 -1.39813811e+00 -1.31297673e+00]\n",
      " [-1.50652052e+00  1.06445364e-01 -1.28440670e+00 -1.31297673e+00]\n",
      " [-1.02184904e+00  1.26346019e+00 -1.34127240e+00 -1.31297673e+00]\n",
      " [-5.37177559e-01  1.95766909e+00 -1.17067529e+00 -1.05003079e+00]\n",
      " [-1.50652052e+00  8.00654259e-01 -1.34127240e+00 -1.18150376e+00]\n",
      " [-1.02184904e+00  8.00654259e-01 -1.28440670e+00 -1.31297673e+00]\n",
      " [-1.74885626e+00 -3.56360566e-01 -1.34127240e+00 -1.31297673e+00]\n",
      " [-1.14301691e+00  1.06445364e-01 -1.28440670e+00 -1.44444970e+00]\n",
      " [-5.37177559e-01  1.49486315e+00 -1.28440670e+00 -1.31297673e+00]\n",
      " [-1.26418478e+00  8.00654259e-01 -1.22754100e+00 -1.31297673e+00]\n",
      " [-1.26418478e+00 -1.24957601e-01 -1.34127240e+00 -1.44444970e+00]\n",
      " [-1.87002413e+00 -1.24957601e-01 -1.51186952e+00 -1.44444970e+00]\n",
      " [-5.25060772e-02  2.18907205e+00 -1.45500381e+00 -1.31297673e+00]\n",
      " [-1.73673948e-01  3.11468391e+00 -1.28440670e+00 -1.05003079e+00]\n",
      " [-5.37177559e-01  1.95766909e+00 -1.39813811e+00 -1.05003079e+00]\n",
      " [-9.00681170e-01  1.03205722e+00 -1.34127240e+00 -1.18150376e+00]\n",
      " [-1.73673948e-01  1.72626612e+00 -1.17067529e+00 -1.18150376e+00]\n",
      " [-9.00681170e-01  1.72626612e+00 -1.28440670e+00 -1.18150376e+00]\n",
      " [-5.37177559e-01  8.00654259e-01 -1.17067529e+00 -1.31297673e+00]\n",
      " [-9.00681170e-01  1.49486315e+00 -1.28440670e+00 -1.05003079e+00]\n",
      " [-1.50652052e+00  1.26346019e+00 -1.56873522e+00 -1.31297673e+00]\n",
      " [-9.00681170e-01  5.69251294e-01 -1.17067529e+00 -9.18557817e-01]\n",
      " [-1.26418478e+00  8.00654259e-01 -1.05694388e+00 -1.31297673e+00]\n",
      " [-1.02184904e+00 -1.24957601e-01 -1.22754100e+00 -1.31297673e+00]\n",
      " [-1.02184904e+00  8.00654259e-01 -1.22754100e+00 -1.05003079e+00]\n",
      " [-7.79513300e-01  1.03205722e+00 -1.28440670e+00 -1.31297673e+00]\n",
      " [-7.79513300e-01  8.00654259e-01 -1.34127240e+00 -1.31297673e+00]\n",
      " [-1.38535265e+00  3.37848329e-01 -1.22754100e+00 -1.31297673e+00]\n",
      " [-1.26418478e+00  1.06445364e-01 -1.22754100e+00 -1.31297673e+00]\n",
      " [-5.37177559e-01  8.00654259e-01 -1.28440670e+00 -1.05003079e+00]\n",
      " [-7.79513300e-01  2.42047502e+00 -1.28440670e+00 -1.44444970e+00]\n",
      " [-4.16009689e-01  2.65187798e+00 -1.34127240e+00 -1.31297673e+00]\n",
      " [-1.14301691e+00  1.06445364e-01 -1.28440670e+00 -1.44444970e+00]\n",
      " [-1.02184904e+00  3.37848329e-01 -1.45500381e+00 -1.31297673e+00]\n",
      " [-4.16009689e-01  1.03205722e+00 -1.39813811e+00 -1.31297673e+00]\n",
      " [-1.14301691e+00  1.06445364e-01 -1.28440670e+00 -1.44444970e+00]\n",
      " [-1.74885626e+00 -1.24957601e-01 -1.39813811e+00 -1.31297673e+00]\n",
      " [-9.00681170e-01  8.00654259e-01 -1.28440670e+00 -1.31297673e+00]\n",
      " [-1.02184904e+00  1.03205722e+00 -1.39813811e+00 -1.18150376e+00]\n",
      " [-1.62768839e+00 -1.74477836e+00 -1.39813811e+00 -1.18150376e+00]\n",
      " [-1.74885626e+00  3.37848329e-01 -1.39813811e+00 -1.31297673e+00]\n",
      " [-1.02184904e+00  1.03205722e+00 -1.22754100e+00 -7.87084847e-01]\n",
      " [-9.00681170e-01  1.72626612e+00 -1.05694388e+00 -1.05003079e+00]\n",
      " [-1.26418478e+00 -1.24957601e-01 -1.34127240e+00 -1.18150376e+00]\n",
      " [-9.00681170e-01  1.72626612e+00 -1.22754100e+00 -1.31297673e+00]\n",
      " [-1.50652052e+00  3.37848329e-01 -1.34127240e+00 -1.31297673e+00]\n",
      " [-6.58345429e-01  1.49486315e+00 -1.28440670e+00 -1.31297673e+00]\n",
      " [-1.02184904e+00  5.69251294e-01 -1.34127240e+00 -1.31297673e+00]\n",
      " [ 1.40150837e+00  3.37848329e-01  5.35295827e-01  2.64698913e-01]\n",
      " [ 6.74501145e-01  3.37848329e-01  4.21564419e-01  3.96171883e-01]\n",
      " [ 1.28034050e+00  1.06445364e-01  6.49027235e-01  3.96171883e-01]\n",
      " [-4.16009689e-01 -1.74477836e+00  1.37235899e-01  1.33225943e-01]\n",
      " [ 7.95669016e-01 -5.87763531e-01  4.78430123e-01  3.96171883e-01]\n",
      " [-1.73673948e-01 -5.87763531e-01  4.21564419e-01  1.33225943e-01]\n",
      " [ 5.53333275e-01  5.69251294e-01  5.35295827e-01  5.27644853e-01]\n",
      " [-1.14301691e+00 -1.51337539e+00 -2.60824029e-01 -2.61192967e-01]\n",
      " [ 9.16836886e-01 -3.56360566e-01  4.78430123e-01  1.33225943e-01]\n",
      " [-7.79513300e-01 -8.19166497e-01  8.03701950e-02  2.64698913e-01]\n",
      " [-1.02184904e+00 -2.43898725e+00 -1.47092621e-01 -2.61192967e-01]\n",
      " [ 6.86617933e-02 -1.24957601e-01  2.50967307e-01  3.96171883e-01]\n",
      " [ 1.89829664e-01 -1.97618132e+00  1.37235899e-01 -2.61192967e-01]\n",
      " [ 3.10997534e-01 -3.56360566e-01  5.35295827e-01  2.64698913e-01]\n",
      " [-2.94841818e-01 -3.56360566e-01 -9.02269170e-02  1.33225943e-01]\n",
      " [ 1.03800476e+00  1.06445364e-01  3.64698715e-01  2.64698913e-01]\n",
      " [-2.94841818e-01 -1.24957601e-01  4.21564419e-01  3.96171883e-01]\n",
      " [-5.25060772e-02 -8.19166497e-01  1.94101603e-01 -2.61192967e-01]\n",
      " [ 4.32165405e-01 -1.97618132e+00  4.21564419e-01  3.96171883e-01]\n",
      " [-2.94841818e-01 -1.28197243e+00  8.03701950e-02 -1.29719997e-01]\n",
      " [ 6.86617933e-02  3.37848329e-01  5.92161531e-01  7.90590793e-01]\n",
      " [ 3.10997534e-01 -5.87763531e-01  1.37235899e-01  1.33225943e-01]\n",
      " [ 5.53333275e-01 -1.28197243e+00  6.49027235e-01  3.96171883e-01]\n",
      " [ 3.10997534e-01 -5.87763531e-01  5.35295827e-01  1.75297293e-03]\n",
      " [ 6.74501145e-01 -3.56360566e-01  3.07833011e-01  1.33225943e-01]\n",
      " [ 9.16836886e-01 -1.24957601e-01  3.64698715e-01  2.64698913e-01]\n",
      " [ 1.15917263e+00 -5.87763531e-01  5.92161531e-01  2.64698913e-01]\n",
      " [ 1.03800476e+00 -1.24957601e-01  7.05892939e-01  6.59117823e-01]\n",
      " [ 1.89829664e-01 -3.56360566e-01  4.21564419e-01  3.96171883e-01]\n",
      " [-1.73673948e-01 -1.05056946e+00 -1.47092621e-01 -2.61192967e-01]\n",
      " [-4.16009689e-01 -1.51337539e+00  2.35044910e-02 -1.29719997e-01]\n",
      " [-4.16009689e-01 -1.51337539e+00 -3.33612130e-02 -2.61192967e-01]\n",
      " [-5.25060772e-02 -8.19166497e-01  8.03701950e-02  1.75297293e-03]\n",
      " [ 1.89829664e-01 -8.19166497e-01  7.62758643e-01  5.27644853e-01]\n",
      " [-5.37177559e-01 -1.24957601e-01  4.21564419e-01  3.96171883e-01]\n",
      " [ 1.89829664e-01  8.00654259e-01  4.21564419e-01  5.27644853e-01]\n",
      " [ 1.03800476e+00  1.06445364e-01  5.35295827e-01  3.96171883e-01]\n",
      " [ 5.53333275e-01 -1.74477836e+00  3.64698715e-01  1.33225943e-01]\n",
      " [-2.94841818e-01 -1.24957601e-01  1.94101603e-01  1.33225943e-01]\n",
      " [-4.16009689e-01 -1.28197243e+00  1.37235899e-01  1.33225943e-01]\n",
      " [-4.16009689e-01 -1.05056946e+00  3.64698715e-01  1.75297293e-03]\n",
      " [ 3.10997534e-01 -1.24957601e-01  4.78430123e-01  2.64698913e-01]\n",
      " [-5.25060772e-02 -1.05056946e+00  1.37235899e-01  1.75297293e-03]\n",
      " [-1.02184904e+00 -1.74477836e+00 -2.60824029e-01 -2.61192967e-01]\n",
      " [-2.94841818e-01 -8.19166497e-01  2.50967307e-01  1.33225943e-01]\n",
      " [-1.73673948e-01 -1.24957601e-01  2.50967307e-01  1.75297293e-03]\n",
      " [-1.73673948e-01 -3.56360566e-01  2.50967307e-01  1.33225943e-01]\n",
      " [ 4.32165405e-01 -3.56360566e-01  3.07833011e-01  1.33225943e-01]\n",
      " [-9.00681170e-01 -1.28197243e+00 -4.31421141e-01 -1.29719997e-01]\n",
      " [-1.73673948e-01 -5.87763531e-01  1.94101603e-01  1.33225943e-01]\n",
      " [ 5.53333275e-01  5.69251294e-01  1.27454998e+00  1.71090158e+00]\n",
      " [-5.25060772e-02 -8.19166497e-01  7.62758643e-01  9.22063763e-01]\n",
      " [ 1.52267624e+00 -1.24957601e-01  1.21768427e+00  1.18500970e+00]\n",
      " [ 5.53333275e-01 -3.56360566e-01  1.04708716e+00  7.90590793e-01]\n",
      " [ 7.95669016e-01 -1.24957601e-01  1.16081857e+00  1.31648267e+00]\n",
      " [ 2.12851559e+00 -1.24957601e-01  1.61574420e+00  1.18500970e+00]\n",
      " [-1.14301691e+00 -1.28197243e+00  4.21564419e-01  6.59117823e-01]\n",
      " [ 1.76501198e+00 -3.56360566e-01  1.44514709e+00  7.90590793e-01]\n",
      " [ 1.03800476e+00 -1.28197243e+00  1.16081857e+00  7.90590793e-01]\n",
      " [ 1.64384411e+00  1.26346019e+00  1.33141568e+00  1.71090158e+00]\n",
      " [ 7.95669016e-01  3.37848329e-01  7.62758643e-01  1.05353673e+00]\n",
      " [ 6.74501145e-01 -8.19166497e-01  8.76490051e-01  9.22063763e-01]\n",
      " [ 1.15917263e+00 -1.24957601e-01  9.90221459e-01  1.18500970e+00]\n",
      " [-1.73673948e-01 -1.28197243e+00  7.05892939e-01  1.05353673e+00]\n",
      " [-5.25060772e-02 -5.87763531e-01  7.62758643e-01  1.57942861e+00]\n",
      " [ 6.74501145e-01  3.37848329e-01  8.76490051e-01  1.44795564e+00]\n",
      " [ 7.95669016e-01 -1.24957601e-01  9.90221459e-01  7.90590793e-01]\n",
      " [ 2.24968346e+00  1.72626612e+00  1.67260991e+00  1.31648267e+00]\n",
      " [ 2.24968346e+00 -1.05056946e+00  1.78634131e+00  1.44795564e+00]\n",
      " [ 1.89829664e-01 -1.97618132e+00  7.05892939e-01  3.96171883e-01]\n",
      " [ 1.28034050e+00  3.37848329e-01  1.10395287e+00  1.44795564e+00]\n",
      " [-2.94841818e-01 -5.87763531e-01  6.49027235e-01  1.05353673e+00]\n",
      " [ 2.24968346e+00 -5.87763531e-01  1.67260991e+00  1.05353673e+00]\n",
      " [ 5.53333275e-01 -8.19166497e-01  6.49027235e-01  7.90590793e-01]\n",
      " [ 1.03800476e+00  5.69251294e-01  1.10395287e+00  1.18500970e+00]\n",
      " [ 1.64384411e+00  3.37848329e-01  1.27454998e+00  7.90590793e-01]\n",
      " [ 4.32165405e-01 -5.87763531e-01  5.92161531e-01  7.90590793e-01]\n",
      " [ 3.10997534e-01 -1.24957601e-01  6.49027235e-01  7.90590793e-01]\n",
      " [ 6.74501145e-01 -5.87763531e-01  1.04708716e+00  1.18500970e+00]\n",
      " [ 1.64384411e+00 -1.24957601e-01  1.16081857e+00  5.27644853e-01]\n",
      " [ 1.88617985e+00 -5.87763531e-01  1.33141568e+00  9.22063763e-01]\n",
      " [ 2.49201920e+00  1.72626612e+00  1.50201279e+00  1.05353673e+00]\n",
      " [ 6.74501145e-01 -5.87763531e-01  1.04708716e+00  1.31648267e+00]\n",
      " [ 5.53333275e-01 -5.87763531e-01  7.62758643e-01  3.96171883e-01]\n",
      " [ 3.10997534e-01 -1.05056946e+00  1.04708716e+00  2.64698913e-01]\n",
      " [ 2.24968346e+00 -1.24957601e-01  1.33141568e+00  1.44795564e+00]\n",
      " [ 5.53333275e-01  8.00654259e-01  1.04708716e+00  1.57942861e+00]\n",
      " [ 6.74501145e-01  1.06445364e-01  9.90221459e-01  7.90590793e-01]\n",
      " [ 1.89829664e-01 -1.24957601e-01  5.92161531e-01  7.90590793e-01]\n",
      " [ 1.28034050e+00  1.06445364e-01  9.33355755e-01  1.18500970e+00]\n",
      " [ 1.03800476e+00  1.06445364e-01  1.04708716e+00  1.57942861e+00]\n",
      " [ 1.28034050e+00  1.06445364e-01  7.62758643e-01  1.44795564e+00]\n",
      " [-5.25060772e-02 -8.19166497e-01  7.62758643e-01  9.22063763e-01]\n",
      " [ 1.15917263e+00  3.37848329e-01  1.21768427e+00  1.44795564e+00]\n",
      " [ 1.03800476e+00  5.69251294e-01  1.10395287e+00  1.71090158e+00]\n",
      " [ 1.03800476e+00 -1.24957601e-01  8.19624347e-01  1.44795564e+00]\n",
      " [ 5.53333275e-01 -1.28197243e+00  7.05892939e-01  9.22063763e-01]\n",
      " [ 7.95669016e-01 -1.24957601e-01  8.19624347e-01  1.05353673e+00]\n",
      " [ 4.32165405e-01  8.00654259e-01  9.33355755e-01  1.44795564e+00]\n",
      " [ 6.86617933e-02 -1.24957601e-01  7.62758643e-01  7.90590793e-01]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- training time: 21.499300479888916 [s] --\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "layers = [X_train.shape[1], 32, 16, 3]\n",
    "FF = neural_network(layers)\n",
    "\n",
    "start_time = time.time()\n",
    "loss, error = FF.train(X_train, y_onehot, epoch = 100, learning_rate = 0.5)\n",
    "print(\"-- training time: %s [s] --\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5001864869643068, 0.5938537301991589, 0.5934863629717966, 0.5911098849334617, 0.5871041975203477, 0.5857350494668473, 0.5846666903698585, 0.584190155728514, 0.5834698646352477, 0.5825527977426481, 0.5816249873088933, 0.5806133951182797, 0.5786155675336393, 0.5708925685383216, 0.5634459861496585, 0.5272320540529674, 0.5422998571584886, 0.5393899871904616, 0.5590148476555046, 0.5660029597912636, 0.5572709707240374, 0.5559935130244571, 0.5606287926168839, 0.5706676261637891, 0.5250163299574213, 0.5488415505103623, 0.6593988553618177, 0.5922130761234657, 0.6713852020797679, 0.6295399844731393, 0.5819973553285149, 0.5840877988360376, 0.5811386126259801, 0.5773138993615223, 0.5750871696182922, 0.5728327642401292, 0.5666557246789822, 0.5675304604037232, 0.5682371363789844, 0.569850250239178, 0.5688679912674359, 0.5669648728825293, 0.5632473209915629, 0.5582109750548954, 0.5573698747126299, 0.5564456181492393, 0.5549815485238402, 0.5548716474606629, 0.5538664902261666, 0.5552337976572299, 0.5567534543949378, 0.5540878848100592, 0.5552995663572055, 0.5447345040757035, 0.5349823774675114, 0.5271493456396202, 0.527859828475971, 0.5415233191040744, 0.5288307856592579, 0.5165010765544057, 0.5389639826501145, 0.5279151516047114, 0.5445551902195445, 0.5658614951225066, 0.5794514646019101, 0.5898775270528863, 0.5949119841917713, 0.636654217040617, 0.7866633382872422, 0.7729788183859768, 0.6963626777374147, 0.6335698713415209, 0.6408508546962586, 0.5645319919725422, 0.546614916726486, 0.5185972922124263, 0.541091534137244, 0.5557127918237869, 0.5500361832980383, 0.6012712037064145, 0.5918530719318076, 0.5906683987412503, 0.5818418375226359, 0.5657185989017506, 0.5558061802021883, 0.6388817162633753, 0.53339856538764, 0.5860401906718299, 0.5751744398286052, 0.5762145550693679, 0.5855437246530664, 0.5844301457350798, 0.5832322990900881, 0.5287464608336333, 0.5951106770636415, 0.6079504063129442, 0.6341991957114237, 0.6104181198968822, 0.5976239719566623, 0.5790357660811426]\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32, 0.4866666666666667, 0.4866666666666667, 0.47333333333333333, 0.4666666666666667, 0.4666666666666667, 0.46, 0.46, 0.4533333333333333, 0.44666666666666666, 0.44666666666666666, 0.44666666666666666, 0.44666666666666666, 0.43333333333333335, 0.41333333333333333, 0.36, 0.38666666666666666, 0.3333333333333333, 0.44, 0.43333333333333335, 0.42, 0.4066666666666667, 0.44666666666666666, 0.42, 0.44, 0.44, 0.5933333333333334, 0.44, 0.7133333333333334, 0.5066666666666667, 0.4666666666666667, 0.49333333333333335, 0.48, 0.4666666666666667, 0.4666666666666667, 0.47333333333333333, 0.44, 0.43333333333333335, 0.42, 0.44666666666666666, 0.44, 0.4266666666666667, 0.4266666666666667, 0.4066666666666667, 0.42, 0.4266666666666667, 0.4266666666666667, 0.43333333333333335, 0.43333333333333335, 0.43333333333333335, 0.43333333333333335, 0.4266666666666667, 0.43333333333333335, 0.4066666666666667, 0.4, 0.38, 0.35333333333333333, 0.4066666666666667, 0.36, 0.3466666666666667, 0.4, 0.36666666666666664, 0.4, 0.44, 0.44, 0.46, 0.49333333333333335, 0.5933333333333334, 0.86, 0.7933333333333333, 0.6933333333333334, 0.58, 0.5733333333333334, 0.44666666666666666, 0.41333333333333333, 0.32, 0.30666666666666664, 0.30666666666666664, 0.3933333333333333, 0.56, 0.36, 0.3333333333333333, 0.34, 0.38, 0.4, 0.52, 0.29333333333333333, 0.26, 0.2866666666666667, 0.32, 0.42, 0.4533333333333333, 0.4866666666666667, 0.37333333333333335, 0.46, 0.41333333333333333, 0.4066666666666667, 0.41333333333333333, 0.41333333333333333, 0.4]\n"
     ]
    }
   ],
   "source": [
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"segundo\"></a>\n",
    "### 2. Comparar back-propagation (BP) de Keras\n",
    "\n",
    "Keras es una de las librerı́as más populares para desarrollar nuevos modelos de redes neuronales o implementar eficientemente modelos conocidos con fines prácticos, puesto que ofrece una interfaz para poder trabajar de una manera mucho mas simple además de permitir también el manejo de configuraciones mas específicas.  \n",
    "Como actividad pedagógica ahora se les pide comparar el algoritmo implementado por ustedes con el de alto nivel de la librería __[keras](https://keras.io/)__ . Se les pedirá comparar sobre el mismo dataset con la misma arquitectura utilizada anteriormente, es decir, dos capas ocultas (con 32 y 16 neuronas respectivamente), 3 neuronas en la capa de salida con función de activación softmax, optimizador Gradiente Descentente (GD) con tasa de aprendizaje fija.\n",
    "\n",
    "<img src=\"https://i.imgur.com/hUjFUDU.png\" width=\"40%\" height=\"40%\" />\n",
    "\n",
    "\n",
    "> a) Defina, a través de la interfaz de keras, la arquitectura de la red, con las funciones de activación para comparar con la sección anterior.\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=X_train.shape[1], activation=\"sigmoid or relu\"))\n",
    "model.add(Dense(16, activation=\"sigmoid or relu\"))\n",
    "model.add(Dense(3, activation=\"softmax\"))\n",
    "```\n",
    "\n",
    "> b) Defina, a través de la interfaz de keras, el optimizador de la red, en conjunto con la función de error, para poder comparar con la sección anterior.\n",
    "```python\n",
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.1),loss=\"categorical_crossentropy or mse\", metrics=[\"accuracy\"])\n",
    "```\n",
    "\n",
    "> c) Entrene (ajuste) los pesos de la red definida mediante keras, reportando los mismos gráficos de la sección anterior para poder comparar. Si hay diferencias en la convergencia del algoritmo ¿A qué podría deverse? si hay una gran diferencia en los tiempos de entrenamiento ¿A qué podría deverse?\n",
    "```python\n",
    "model.fit(X_train, y_onehot, epochs=100, batch_size=1, verbose=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=X_train.shape[1], activation=\"sigmoid\"))\n",
    "model.add(Dense(16, activation=\"sigmoid\"))\n",
    "model.add(Dense(3, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.1),loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 1.0894 - acc: 0.4267\n",
      "Epoch 2/100\n",
      "150/150 [==============================] - 0s 875us/step - loss: 0.5978 - acc: 0.7533\n",
      "Epoch 3/100\n",
      "150/150 [==============================] - 0s 860us/step - loss: 0.3969 - acc: 0.8733\n",
      "Epoch 4/100\n",
      "150/150 [==============================] - 0s 918us/step - loss: 0.2866 - acc: 0.9000\n",
      "Epoch 5/100\n",
      "150/150 [==============================] - 0s 887us/step - loss: 0.2328 - acc: 0.9267\n",
      "Epoch 6/100\n",
      "150/150 [==============================] - 0s 865us/step - loss: 0.1866 - acc: 0.9467\n",
      "Epoch 7/100\n",
      "150/150 [==============================] - 0s 873us/step - loss: 0.1563 - acc: 0.9533\n",
      "Epoch 8/100\n",
      "150/150 [==============================] - 0s 904us/step - loss: 0.1333 - acc: 0.9667\n",
      "Epoch 9/100\n",
      "150/150 [==============================] - 0s 859us/step - loss: 0.1198 - acc: 0.9667\n",
      "Epoch 10/100\n",
      "150/150 [==============================] - 0s 874us/step - loss: 0.1041 - acc: 0.9733\n",
      "Epoch 11/100\n",
      "150/150 [==============================] - 0s 873us/step - loss: 0.0990 - acc: 0.9533\n",
      "Epoch 12/100\n",
      "150/150 [==============================] - 0s 852us/step - loss: 0.1031 - acc: 0.9533\n",
      "Epoch 13/100\n",
      "150/150 [==============================] - 0s 914us/step - loss: 0.0974 - acc: 0.9533\n",
      "Epoch 14/100\n",
      "150/150 [==============================] - 0s 850us/step - loss: 0.0749 - acc: 0.9800\n",
      "Epoch 15/100\n",
      "150/150 [==============================] - 0s 870us/step - loss: 0.0826 - acc: 0.9667\n",
      "Epoch 16/100\n",
      "150/150 [==============================] - 0s 847us/step - loss: 0.0971 - acc: 0.9600\n",
      "Epoch 17/100\n",
      "150/150 [==============================] - 0s 877us/step - loss: 0.0998 - acc: 0.9533\n",
      "Epoch 18/100\n",
      "150/150 [==============================] - 0s 889us/step - loss: 0.0789 - acc: 0.9667\n",
      "Epoch 19/100\n",
      "150/150 [==============================] - 0s 863us/step - loss: 0.0806 - acc: 0.9667\n",
      "Epoch 20/100\n",
      "150/150 [==============================] - 0s 887us/step - loss: 0.0906 - acc: 0.9533\n",
      "Epoch 21/100\n",
      "150/150 [==============================] - 0s 864us/step - loss: 0.0612 - acc: 0.9800\n",
      "Epoch 22/100\n",
      "150/150 [==============================] - 0s 836us/step - loss: 0.0974 - acc: 0.9600\n",
      "Epoch 23/100\n",
      "150/150 [==============================] - 0s 859us/step - loss: 0.0912 - acc: 0.9667\n",
      "Epoch 24/100\n",
      "150/150 [==============================] - 0s 899us/step - loss: 0.0716 - acc: 0.9733\n",
      "Epoch 25/100\n",
      "150/150 [==============================] - 0s 856us/step - loss: 0.0829 - acc: 0.9467\n",
      "Epoch 26/100\n",
      "150/150 [==============================] - 0s 870us/step - loss: 0.0553 - acc: 0.9800\n",
      "Epoch 27/100\n",
      "150/150 [==============================] - 0s 862us/step - loss: 0.0596 - acc: 0.9800\n",
      "Epoch 28/100\n",
      "150/150 [==============================] - 0s 873us/step - loss: 0.0862 - acc: 0.9533\n",
      "Epoch 29/100\n",
      "150/150 [==============================] - 0s 874us/step - loss: 0.0752 - acc: 0.9600\n",
      "Epoch 30/100\n",
      "150/150 [==============================] - 0s 876us/step - loss: 0.0733 - acc: 0.9667\n",
      "Epoch 31/100\n",
      "150/150 [==============================] - 0s 847us/step - loss: 0.0797 - acc: 0.9733\n",
      "Epoch 32/100\n",
      "150/150 [==============================] - 0s 880us/step - loss: 0.0732 - acc: 0.9667\n",
      "Epoch 33/100\n",
      "150/150 [==============================] - 0s 866us/step - loss: 0.0970 - acc: 0.9467\n",
      "Epoch 34/100\n",
      "150/150 [==============================] - 0s 871us/step - loss: 0.0732 - acc: 0.9667\n",
      "Epoch 35/100\n",
      "150/150 [==============================] - 0s 855us/step - loss: 0.0707 - acc: 0.9733\n",
      "Epoch 36/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0763 - acc: 0.9733\n",
      "Epoch 37/100\n",
      "150/150 [==============================] - 0s 855us/step - loss: 0.0611 - acc: 0.9800\n",
      "Epoch 38/100\n",
      "150/150 [==============================] - 0s 857us/step - loss: 0.0617 - acc: 0.9733\n",
      "Epoch 39/100\n",
      "150/150 [==============================] - 0s 862us/step - loss: 0.0687 - acc: 0.9600\n",
      "Epoch 40/100\n",
      "150/150 [==============================] - 0s 868us/step - loss: 0.0701 - acc: 0.9600\n",
      "Epoch 41/100\n",
      "150/150 [==============================] - 0s 917us/step - loss: 0.0761 - acc: 0.9667\n",
      "Epoch 42/100\n",
      "150/150 [==============================] - 0s 880us/step - loss: 0.0826 - acc: 0.9667\n",
      "Epoch 43/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0703 - acc: 0.9600\n",
      "Epoch 44/100\n",
      "150/150 [==============================] - 0s 982us/step - loss: 0.0514 - acc: 0.9733\n",
      "Epoch 45/100\n",
      "150/150 [==============================] - 0s 925us/step - loss: 0.0604 - acc: 0.9733\n",
      "Epoch 46/100\n",
      "150/150 [==============================] - 0s 953us/step - loss: 0.0571 - acc: 0.9867\n",
      "Epoch 47/100\n",
      "150/150 [==============================] - 0s 889us/step - loss: 0.0616 - acc: 0.9600\n",
      "Epoch 48/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0632 - acc: 0.9733\n",
      "Epoch 49/100\n",
      "150/150 [==============================] - 0s 871us/step - loss: 0.0741 - acc: 0.9733\n",
      "Epoch 50/100\n",
      "150/150 [==============================] - 0s 889us/step - loss: 0.0523 - acc: 0.9733\n",
      "Epoch 51/100\n",
      "150/150 [==============================] - 0s 866us/step - loss: 0.0815 - acc: 0.9800\n",
      "Epoch 52/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0692 - acc: 0.9667\n",
      "Epoch 53/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0571 - acc: 0.9800\n",
      "Epoch 54/100\n",
      "150/150 [==============================] - 0s 1000us/step - loss: 0.0711 - acc: 0.9800\n",
      "Epoch 55/100\n",
      "150/150 [==============================] - 0s 997us/step - loss: 0.0705 - acc: 0.9600\n",
      "Epoch 56/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0752 - acc: 0.9733\n",
      "Epoch 57/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0536 - acc: 0.9800\n",
      "Epoch 58/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0554 - acc: 0.9800\n",
      "Epoch 59/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0508 - acc: 0.9667\n",
      "Epoch 60/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0763 - acc: 0.9733\n",
      "Epoch 61/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0547 - acc: 0.9667\n",
      "Epoch 62/100\n",
      "150/150 [==============================] - 0s 892us/step - loss: 0.0747 - acc: 0.9733\n",
      "Epoch 63/100\n",
      "150/150 [==============================] - 0s 844us/step - loss: 0.0659 - acc: 0.9800\n",
      "Epoch 64/100\n",
      "150/150 [==============================] - 0s 970us/step - loss: 0.0729 - acc: 0.9667\n",
      "Epoch 65/100\n",
      "150/150 [==============================] - 0s 968us/step - loss: 0.0707 - acc: 0.9667\n",
      "Epoch 66/100\n",
      "150/150 [==============================] - 0s 988us/step - loss: 0.0667 - acc: 0.9667\n",
      "Epoch 67/100\n",
      "150/150 [==============================] - 0s 999us/step - loss: 0.0510 - acc: 0.9800\n",
      "Epoch 68/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0603 - acc: 0.9733\n",
      "Epoch 69/100\n",
      "150/150 [==============================] - 0s 941us/step - loss: 0.0504 - acc: 0.9667\n",
      "Epoch 70/100\n",
      "150/150 [==============================] - 0s 915us/step - loss: 0.0727 - acc: 0.9667\n",
      "Epoch 71/100\n",
      "150/150 [==============================] - 0s 935us/step - loss: 0.0519 - acc: 0.9667\n",
      "Epoch 72/100\n",
      "150/150 [==============================] - 0s 985us/step - loss: 0.0595 - acc: 0.9667\n",
      "Epoch 73/100\n",
      "150/150 [==============================] - 0s 972us/step - loss: 0.0584 - acc: 0.9667\n",
      "Epoch 74/100\n",
      "150/150 [==============================] - 0s 967us/step - loss: 0.0662 - acc: 0.9667\n",
      "Epoch 75/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0643 - acc: 0.9800\n",
      "Epoch 76/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0648 - acc: 0.9733\n",
      "Epoch 77/100\n",
      "150/150 [==============================] - 0s 962us/step - loss: 0.0593 - acc: 0.9800\n",
      "Epoch 78/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0629 - acc: 0.9733\n",
      "Epoch 79/100\n",
      "150/150 [==============================] - 0s 937us/step - loss: 0.0621 - acc: 0.9800\n",
      "Epoch 80/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0588 - acc: 0.9733\n",
      "Epoch 81/100\n",
      "150/150 [==============================] - 0s 949us/step - loss: 0.0591 - acc: 0.9800\n",
      "Epoch 82/100\n",
      "150/150 [==============================] - 0s 878us/step - loss: 0.0680 - acc: 0.9600\n",
      "Epoch 83/100\n",
      "150/150 [==============================] - 0s 891us/step - loss: 0.0596 - acc: 0.9667\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 936us/step - loss: 0.0540 - acc: 0.9733\n",
      "Epoch 85/100\n",
      "150/150 [==============================] - 0s 858us/step - loss: 0.0659 - acc: 0.9733\n",
      "Epoch 86/100\n",
      "150/150 [==============================] - 0s 887us/step - loss: 0.0536 - acc: 0.9800\n",
      "Epoch 87/100\n",
      "150/150 [==============================] - 0s 896us/step - loss: 0.0584 - acc: 0.9733\n",
      "Epoch 88/100\n",
      "150/150 [==============================] - 0s 949us/step - loss: 0.0741 - acc: 0.9533\n",
      "Epoch 89/100\n",
      "150/150 [==============================] - 0s 984us/step - loss: 0.0640 - acc: 0.9733\n",
      "Epoch 90/100\n",
      "150/150 [==============================] - 0s 962us/step - loss: 0.0521 - acc: 0.9733\n",
      "Epoch 91/100\n",
      "150/150 [==============================] - 0s 954us/step - loss: 0.0603 - acc: 0.9733\n",
      "Epoch 92/100\n",
      "150/150 [==============================] - 0s 959us/step - loss: 0.0565 - acc: 0.9867\n",
      "Epoch 93/100\n",
      "150/150 [==============================] - 0s 940us/step - loss: 0.0596 - acc: 0.9600\n",
      "Epoch 94/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0491 - acc: 0.9867\n",
      "Epoch 95/100\n",
      "150/150 [==============================] - 0s 860us/step - loss: 0.0657 - acc: 0.9733\n",
      "Epoch 96/100\n",
      "150/150 [==============================] - 0s 859us/step - loss: 0.0614 - acc: 0.9733\n",
      "Epoch 97/100\n",
      "150/150 [==============================] - 0s 882us/step - loss: 0.0510 - acc: 0.9867\n",
      "Epoch 98/100\n",
      "150/150 [==============================] - 0s 862us/step - loss: 0.0647 - acc: 0.9600\n",
      "Epoch 99/100\n",
      "150/150 [==============================] - 0s 852us/step - loss: 0.0534 - acc: 0.9733\n",
      "Epoch 100/100\n",
      "150/150 [==============================] - 0s 852us/step - loss: 0.0635 - acc: 0.9667\n",
      "-- training time: 14.364876508712769 [s] --\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "history = model.fit(X_train, y_onehot, epochs=100, batch_size=1, verbose=1)\n",
    "print(\"-- training time: %s [s] --\" % (time.time() - start_time))\n",
    "\n",
    "error = list(map(lambda x: 1 - x, history.history['acc']))\n",
    "loss = history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.plot(error, label='Acc')\n",
    "#plt.title('')\n",
    "plt.ylabel('Classification error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(loss, label='Acc')\n",
    "#plt.title('')\n",
    "#plt.ylabel('Classification error')\n",
    "#plt.xlabel('Epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- error clasificacion vs epochs (solo con entrenamiento)\n",
    "- Grafique también la evolución de la función objetivo utilizada para el entrenamiento\n",
    "- Además de reportar el tiempo de entrenamiento mediante el algoritmo implementado. \n",
    "-------------------------------------\n",
    "- Loss: valor escalar que se trata de minimizar durante el entrenamiento de un modelo. Mientras menos sea , más cercanas son las predicciones a las etiquetas verdaderas.\n",
    "- Epoch es una pasada completa sobre todos los datos de entrenamiento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 72us/step\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris Setosa       1.00      1.00      1.00        50\n",
      "Iris Versicolor       0.98      0.98      0.98        50\n",
      " Iris Virginica       0.98      0.98      0.98        50\n",
      "\n",
      "    avg / total       0.99      0.99      0.99       150\n",
      "\n",
      "Matriz de confusión\n",
      "[[50  0  0]\n",
      " [ 0 49  1]\n",
      " [ 0  1 49]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "predictions = model.predict(X_train, verbose=1)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "\n",
    "target_names = [\"Iris Setosa\", \"Iris Versicolor\", \"Iris Virginica\"]\n",
    "print(classification_report(y_train, y_pred, target_names=target_names))\n",
    "print(\"Matriz de confusión\")\n",
    "print(confusion_matrix(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) La matriz de confusión (o error matrix) es una herramienta que permite visualizar el desempeño de un algoritmo en machine learning, especialmente con algoritmos supervisados (en el caso de no supervisados se utiliza la matriz de matching). Cada fila de la matriz representa las instancias en una clase predecida, mientras que las columnas representan las instancias en una clase real. Tiene un especial uso para analizar cuándo un clasificador tiene confusión entre dos clases. Para este experimento se utilizará la matriz proporcionada por la librería *sklearn*.\n",
    "\n",
    "Antes de mostrar la matriz de confusión se muestra el reporte de clasificación de la red neuronal. Esta información ayudará para complementar los resultados de la matriz de confusión. El reporte de clasificación muestra las siguientes métricas para cada clase o etiqueta:\n",
    "\n",
    "- *Precision*: Capacidad del clasificador de no etiquetar como positiva una muestra que es negativa, y viceversa.\n",
    "- *Recall*: Capacidad del clasificador para encontrar todas las muestras positivas/relevantes.\n",
    "- *F1-Score*: Puede interpretarse como una media armónica ponderada de precision y recall.\n",
    "- *Support*: Cantidad de ocurrencias de cada clase.\n",
    "\n",
    "En el ejemplo anterior se puede ver como la red es capaz de clasificar los datos pertenecientes a la clase Iris Setosa, con un 100% de *precision* y *recall*. En el caso de la segunda clase, Iris Versicolor, se logra clasificar correctamente 49 datos de cincuenta, teniendo un error de 1 caso en que se le asigna la etiqueta de la tercera clase. Para la tercera clase,  se clasifica mal un dato con la clase dos y etiqueta correctamente 49 casos. En conclusión, la red se comporta bien como clasificador, etiquetando mal una pequeña cantidad de los datos. Para la clase dos y tres el valor de *precision* y *recall* disminuye y es igual pues cometen el mismo error de clasificación pero aciertan en el mismo número de casos. Esto puede deberse a puntos en el espacio es tienen características muy similares a los datos de la tercera clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tercero\"></a>\n",
    "### 3. Verificación numérica del gradiente para una componente\n",
    "\n",
    "En esta sección deberá verificar numéricamente el gradiente para los parámetros del modelo (que en este caso son los pesos de la red), que hasta ahora a definido de manera analítica en su programa, por ejemplo la derivada de $x^2$ es $2x$. Ahora deberá verificar estos cálculos usando la definición de gradiente.\n",
    "\n",
    "$$ \\nabla_{w} Loss = \\lim_{\\epsilon \\rightarrow 0} \\frac{Loss(w+ \\epsilon)-Loss(w)}{\\epsilon} $$\n",
    "\n",
    "Debido a que el *forward propagation* es relativamente fácil de implementar, se puede confiar en que se realizó de manera correcta, por lo que el cómputo del error (*loss*) debería ser correcto. Esto significa que podemos verificar el gradiente o la derivada analítica del error $\\frac{\\partial Loss}{\\partial w}$ comprobando que el resultado obtenido es similar (dentro de una tolerancia numérica, por ejemplo $10^6$) al valor que obtenemos aplicando la fórmula anterior. Naturalmente interpretaremos $\\lim_{\\epsilon \\rightarrow 0}$ como un valor \"*suficientemente pequeño*\" de $\\epsilon$.\n",
    "\n",
    "\n",
    "> a) Para un peso escogido aleatoriamente entre la primera capa de la red (*input*) y la primera capa oculta, calcule el valor del gradiente de la función de error para ambas funciones utilizadas (ayúdese mediante las funciones de *backward pass* implementadas anteriormente), luego compare y verifique con el valor numérico del gradiente mediante el procedimiento explicado anteriormente.\n",
    "\n",
    "> b) Vuelva a verificar el valor del gradiente para otros dos pesos escodigos aleatoriamente en la primera operación de la red. Compare y concluya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cuarto\"></a>\n",
    "### 4. Implementar *momentum* como variante\n",
    "\n",
    "En esta sección deberá construir, sin usar librerı́as, excepto eventualmente *numpy* para implementar operaciones básicas de algebra lineal, una variante del programa definido anteriormente ([sección 1](#primero)) que entrene la red utilizando *momentum* clásico.\n",
    "\n",
    "$$ v^{(t+1)} \\leftarrow \\mu v^{(t)} - \\eta \\nabla_{w^{(t)}} Loss \\\\\n",
    "w^{(t+1)} \\leftarrow w^{(t)} + v^{(t+1)}\n",
    "$$\n",
    "\n",
    "> *Sutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013, February). On the importance of initialization and momentum in deep learning. In International conference on machine learning (pp. 1139-1147).*\n",
    "\n",
    "\n",
    "Demuestre que su programa funciona en el mismo problema de clasificación presentado anteriormente, para esto, además deberá construir un gráfico de la función de error o pérdida (*loss*) *vs* el número de *epochs* y comentar/analizar la convergencia. ¿Es una mejora significativa? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
